# -*- coding: utf-8 -*-
"""finetune_all.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iPa4Vvj6p_QTgS_AveAdqyLhkI8Mk4fW
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/Polyp-SAM (finetunned)/scripts

!pip install monai
!pip install segment-anything

import torch
import torchvision
print("PyTorch version:", torch.__version__)
print("Torchvision version:", torchvision.__version__)
print("CUDA is available:", torch.cuda.is_available())
import sys
import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
import json
from monai.metrics import DiceMetric, MeanIoU, SurfaceDiceMetric, SSIMMetric, GeneralizedDiceScore
from segment_anything.utils.transforms import ResizeLongestSide
from collections import defaultdict
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from sklearn.model_selection import train_test_split
from monai.losses import GeneralizedDiceLoss, DiceLoss, GeneralizedDiceFocalLoss
from monai.metrics import DiceMetric, GeneralizedDiceScore
from LinearWarmupCosine import LinearWarmupCosineAnnealingLR

def show_anns(anns):
    if len(anns) == 0:
        return
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)
    polygons = []
    color = []
    for ann in sorted_anns:
        m = ann['segmentation']
        img = np.ones((m.shape[0], m.shape[1], 3))
        color_mask = np.random.random((1, 3)).tolist()[0]
        for i in range(3):
            img[:,:,i] = color_mask[i]
        ax.imshow(np.dstack((img, m*0.35)))

image_folder = '/content/drive/MyDrive/img-seg/TestDataset/CVC-300/images'
mask_folder =  '/content/drive/MyDrive/img-seg/TestDataset/CVC-300/masks'
save_folder = '/content/drive/MyDrive/Polyp-SAM/scripts/masks_output'
os.makedirs(save_folder, exist_ok = True)

image_path = []
mask_path = []

for root, dirs, files in os.walk(image_folder, topdown=False): #finds MRI files
    for name in files:
        if name.endswith(".png"):
            apath=os.path.join(root, name)
            image_path.append(apath)

for root, dirs, files in os.walk(mask_folder, topdown=False): #finds MRI files
    for name in files:
        if name.endswith(".png"):
            apath=os.path.join(root, name)
            mask_path.append(apath)

print(image_path[-1], mask_path[-1])

# with open('D:\Yuheng Li\Segment Anything\kvasir-seg\\kavsir_bboxes.json') as f:
#     labels = json.load(f)

X_train, X_test, y_train, y_test = train_test_split(image_path, mask_path, test_size=0.2, random_state=42)

print(X_train[-1], y_train[-1])

!wget https://huggingface.co/datasets/Gourieff/ReActor/resolve/main/models/sams/sam_vit_b_01ec64.pth?download=true -O sam_vit_b_01ec64.pth

import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor

# sam_checkpoint = "sam_vit_h_4b8939.pth"
# model_type = "vit_h"

sam_checkpoint = "/content/drive/MyDrive/Polyp-SAM (finetunned)/scripts/sam_vit_b_01ec64.pth"
model_type = "vit_b"

# sam_checkpoint = "sam_vit_l_0b3195.pth"
# model_type = "vit_l"


device = "cuda"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

transform = ResizeLongestSide(sam.image_encoder.img_size)

def extract_bboxes(mask, num_instances):

    """Compute bounding boxes from masks.

    mask: [height, width, num_instances]. Mask pixels are either 1 or 0.



    Returns: bbox array [num_instances, (y1, x1, y2, x2)].

    """

    boxes = np.zeros([num_instances, 4], dtype=np.int32)

    for i in range(num_instances):

        m = mask

        # Bounding box.

        horizontal_indicies = np.where(np.any(m, axis=0))[0]

#         print("np.any(m, axis=0)",np.any(m, axis=0))

#         print("p.where(np.any(m, axis=0))",np.where(np.any(m, axis=0)))

        vertical_indicies = np.where(np.any(m, axis=1))[0]

        if horizontal_indicies.shape[0]:

            x1, x2 = horizontal_indicies[[0, -1]]

            y1, y2 = vertical_indicies[[0, -1]]

            # x2 and y2 should not be part of the box. Increment by 1.

            x2 += 1

            y2 += 1

        else:

            # No mask for this instance. Might happen due to

            # resizing or cropping. Set bbox to zeros

            x1, x2, y1, y2 = 0, 0, 0, 0

        boxes[i] = np.array([y1, x1, y2, x2])

    return boxes.astype(np.int32)

# coco mask style dataloader

class ColonDataset(Dataset):
    def __init__(self, image_path, mask_path, image_size):
        self.image_path = image_path
        self.mask_path = mask_path
        self.image_size = image_size

        # TODO: use ResizeLongestSide and pad to square
        self.to_tensor = transforms.ToTensor()
        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])

    def __len__(self):
        return len(self.image_path)

    def __getitem__(self, index):
        filename = os.path.basename(self.image_path[index])
        idx = os.path.splitext(filename)[0]  # Remove the file extension
        image = cv2.imread(self.image_path[index])
        gt = cv2.imread(self.mask_path[index])
        gt = cv2.cvtColor(gt, cv2.COLOR_BGR2GRAY) / 255
        gt = gt.astype('float32')

        bbox_arr = extract_bboxes(gt, 1)

        gt_resized = cv2.resize(gt, (1024, 1024), cv2.INTER_NEAREST)
        gt_resized = torch.as_tensor(gt_resized > 0).long()

        gt = torch.from_numpy(gt)
        gt_binary_mask = torch.as_tensor(gt > 0).long()

        transform = ResizeLongestSide(self.image_size)
        input_image = transform.apply_image(image)
        input_image =  cv2.resize(input_image, (1024, 1024), cv2.INTER_CUBIC)
        input_image= self.to_tensor(input_image)

        # input_image= self.normalize(input_image)
#         print(input_image.shape)
#         plt.figure()
#         plt.imshow(input_image[0])
#         print('before preprcoess', torch.max(input_image[0]), torch.min(input_image[0]))
        # input_image = sam.preprocess(input_image.to('cuda:0')).detach().cpu()
#         print('after preprcoess', torch.max(input_image[0]), torch.min(input_image[0]))
#         input_image = cv2.resize(input_image.numpy(), (1024, 1024), cv2.INTER_CUBIC)

#         plt.figure()
#         plt.imshow(input_image[0])

        original_image_size = image.shape[:2]
        input_size = tuple(input_image.shape[-2:])

        return input_image, np.array(bbox_arr), gt_binary_mask, gt_resized, original_image_size, input_size


def my_collate(batch):

    images, bboxes, masks, gt_resized, original_image_size, input_size = zip(*batch)
    images = torch.stack(images, dim=0)
    gt_resized = torch.stack(gt_resized, dim=0)

    masks = [m for m in masks]
    bboxes = [m for m in bboxes]
    original_image_size = [m for m in original_image_size]
    input_size = [m for m in input_size]

    return images, bboxes, masks, gt_resized, original_image_size, input_size

train_dataset = ColonDataset(X_train, y_train, sam.image_encoder.img_size)
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn = my_collate)

val_dataset = ColonDataset(X_test, y_test, sam.image_encoder.img_size)
val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=True, collate_fn = my_collate)

from matplotlib.patches import Rectangle

for batch in train_dataloader:

    img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]

    for i in range(len(mask)):
        print(original_image_size[i], bbox[i])
        orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]
#         col_x1, col_x2 = bbox[i][:,0] * 1024/orig_x, bbox[i][:,2]* 1024/orig_x
#         col_y1, col_y2 = bbox[i][:,1]* 1024/orig_y, bbox[i][:,3]* 1024/orig_y
        col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y
        col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x

        box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()
        print(box)
        print(img.shape)
        img = img[0]
        gt_resized = gt_resized[0]
        print(img.max(), img.min())
        print(box.shape)
        print(img.shape, gt_resized.shape)
        plt.figure(figsize = (5,5))
        plt.imshow(mask[0])
        plt.figure(figsize = (5,5))
        plt.imshow(gt_resized)
        fig, ax = plt.subplots(figsize = (5,5))
        ax.set_xlim(0, 1024)
        ax.set_ylim(0, 1024)
        plt.gca().invert_yaxis()
        rect = Rectangle((col_x1[0], col_y1[0]), col_x2[0]-col_x1[0], col_y2[0]-col_y1[0], linewidth=2, edgecolor='r', facecolor='none')
        ax.add_patch(rect)
        plt.figure(figsize = (5,5))
        plt.imshow(img[0])
        print(bbox[i])

        # show the plot
        plt.show()

        break
    break

num_epochs = 50
# Set up the optimizer, hyperparameter tuning will improve performance here
lr = 4e-6
wd = 1e-4
torch.backends.cudnn.benchmark = True
parameters = list(sam.mask_decoder.parameters()) + list(sam.image_encoder.parameters()) + list(sam.prompt_encoder.parameters())
scaler = torch.cuda.amp.GradScaler()
optimizer = torch.optim.AdamW(parameters, lr=lr, weight_decay=wd)
scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=30, max_epochs=num_epochs, warmup_start_lr=5e-7, eta_min = 1e-6)

from monai.losses import GeneralizedDiceLoss, DiceLoss, GeneralizedDiceFocalLoss

#root_dir = 'https://drive.google.com/drive/folders/19aE_82EfVYQPJM4IUxLOyHPDdxMz1kuD?usp=drive_link'
root_dir = '/content/drive/MyDrive/Polyp-SAM (finetunned)/scripts/https_/drive.google.com/drive/folders/19aE_82EfVYQPJM4IUxLOyHPDdxMz1kuD_usp=drive_link'

os.makedirs(root_dir, exist_ok = True)
# loss_fn = GeneralizedDiceLoss(to_onehot_y=False)
loss_fn = DiceLoss(to_onehot_y=False)


from statistics import mean

from tqdm import tqdm
from torch.nn.functional import threshold, normalize

losses = []
dice_score = []
gd_score = []

best_dice = -1
best_gd = -1
best_score = -1

for epoch in range(num_epochs):
    epoch_losses = []

    for batch in train_dataloader:

        img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]

        batch_loss = 0

        for i in range(len(mask)):
            with torch.cuda.amp.autocast():

                image_embedding = sam.image_encoder(img[i].unsqueeze(0).to(device))

                orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]
                col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y
                col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x

                box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()

                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)

                sparse_embeddings, dense_embeddings = sam.prompt_encoder(
                  points=None,
                  boxes=box_torch,
                  masks=None,
                )

                low_res_masks, iou_predictions = sam.mask_decoder(
                  image_embeddings=image_embedding,
                  image_pe=sam.prompt_encoder.get_dense_pe(),
                  sparse_prompt_embeddings=sparse_embeddings,
                  dense_prompt_embeddings=dense_embeddings,
                  multimask_output=False,
                )

                upscaled_masks = sam.postprocess_masks(low_res_masks, input_size[i], original_image_size[i]).to(device)

                binary_mask = torch.sigmoid(upscaled_masks)

                gt_binary_mask = mask[i].to(device)

                if binary_mask.size()[0] > 1:
                    binary_mask = torch.unsqueeze(torch.sum(binary_mask, 0) / binary_mask.size()[0],0)

                loss = loss_fn(binary_mask[0], gt_binary_mask.unsqueeze(0)) / len(mask)
                scaler.scale(loss).backward()
                batch_loss += loss.item()

#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
        scaler.step(optimizer)
        scaler.update()
        epoch_losses.append(batch_loss / len(mask))
        optimizer.zero_grad()
    scheduler.step()
    losses.append(mean(epoch_losses))
    print(f'EPOCH: {epoch}')
    print(f'Mean loss: {mean(epoch_losses)}')


    with torch.no_grad():
        batch_dice = []
        batch_gd = []

        for batch in val_dataloader:

            img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]

            dice = DiceMetric()
            gd =  GeneralizedDiceScore()

            for i in range(len(mask)):
                image_embedding = sam.image_encoder(img[i].unsqueeze(0).to(device))

                orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]
                col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y
                col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x

                box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()

                num_masks = box.shape[0]
                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)
                sparse_embeddings, dense_embeddings = sam.prompt_encoder(
                  points=None,
                  boxes= box_torch,
                  masks = None
                )

                low_res_masks, iou_predictions = sam.mask_decoder(
                  image_embeddings=image_embedding,
                  image_pe=sam.prompt_encoder.get_dense_pe(),
                  sparse_prompt_embeddings=sparse_embeddings,
                  dense_prompt_embeddings=dense_embeddings,
                  multimask_output=False
                )

                upscaled_masks = sam.postprocess_masks(low_res_masks, input_size[i], original_image_size[i])

                binary_mask = torch.sigmoid(upscaled_masks.detach().cpu())
                binary_mask =  (binary_mask>0.5).float()

                gt_binary_mask = mask[i].detach().cpu()

                if binary_mask.size()[0] > 1:
                    binary_mask = torch.unsqueeze(torch.sum(binary_mask, 0) / binary_mask.size()[0],0)

                dice.reset()
                gd.reset()

                dice(binary_mask[0,:], gt_binary_mask.unsqueeze(0))
                gd(binary_mask[0,:], gt_binary_mask.unsqueeze(0))
                final_dice = dice.aggregate().numpy()[0]
                final_gd = gd.aggregate().numpy()[0]
                batch_dice.append(final_dice)
                batch_gd.append(final_gd)

        if (sum(batch_dice) / len(batch_dice)) > best_dice:
            best_dice = sum(batch_dice) / len(batch_dice)
            torch.save(
                sam.mask_decoder.state_dict(),
                os.path.join(root_dir, "dec_best_dice_model_DL.pth"),
            )
            torch.save(
                sam.image_encoder.state_dict(),
                os.path.join(root_dir, "img_enc_best_dice_model_DL.pth"),
            )
            torch.save(
                sam.prompt_encoder.state_dict(),
                os.path.join(root_dir, "prompt_enc_best_dice_model_DL.pth"),
            )

            print("saved new best dice model")

        if (sum(batch_gd) / len(batch_gd)) > best_gd:
            best_gd = sum(batch_gd) / len(batch_gd)
            torch.save(
                sam.mask_decoder.state_dict(),
                os.path.join(root_dir, "dec_best_GD_model_DL.pth"),
            )
            torch.save(
                sam.image_encoder.state_dict(),
                os.path.join(root_dir, "img_enc_best_GD_model_DL.pth"),
            )
            torch.save(
                sam.prompt_encoder.state_dict(),
                os.path.join(root_dir, "prompt_enc_best_GD_model_DL.pth"),
            )

            print("saved new best GD model")

        dice_score.append(sum(batch_dice) / len(batch_dice))
        gd_score.append(sum(batch_gd) / len(batch_gd))


    print(f'Mean val dice: {dice_score[-1]}')
    print(f'Mean val gd: {gd_score[-1]}')

plt.figure("train", (18, 6))
plt.subplot(1, 3, 1)
plt.title("Epoch Average Loss")
x = [i + 1 for i in range(len(losses))]
y = losses
plt.xlabel("epoch")
plt.plot(x, y, color="red")
plt.subplot(1, 3, 2)
plt.title("Val Mean Dice")
x = [(i + 1) for i in range(len(dice_score))]
y = dice_score
plt.xlabel("epoch")
plt.plot(x, y, color="green")
plt.subplot(1, 3, 3)
plt.title("Val Generalized Dice")
x = [(i + 1) for i in range(len(gd_score))]
y = gd_score
plt.xlabel("epoch")
plt.plot(x, y, color="green")
plt.show()



max_epochs = 50
optimizer = torch.optim.AdamW(parameters, lr=lr, weight_decay=wd)
scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=30, max_epochs=num_epochs, warmup_start_lr=5e-7, eta_min = 1e-6)

lr = []

for i in range(max_epochs):
    scheduler.step()
    lr.append(optimizer.param_groups[0]['lr'])

plt.figure("train", (6, 6))
plt.title("Epoch LR")
x = [i + 1 for i in range(max_epochs)]
y = lr
plt.xlabel("epoch")
plt.plot(x, y, color="red")

model_path = root_dir

sam.prompt_encoder.load_state_dict(torch.load(os.path.join(model_path, "prompt_enc_best_dice_model_DL.pth")))
sam.image_encoder.load_state_dict(torch.load(os.path.join(model_path, "img_enc_best_dice_model_DL.pth")))
sam.mask_decoder.load_state_dict(torch.load(os.path.join(model_path, "dec_best_dice_model_DL.pth")))
sam.eval()

with torch.no_grad():
    batch_dice = []
    batch_gd = []
    batch_iou = []

    for batch in val_dataloader:

        img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]

        dice = DiceMetric()
        gd =  GeneralizedDiceScore()
        iou = MeanIoU()

        for i in range(len(mask)):
            image_embedding = sam.image_encoder(img[i].unsqueeze(0).to(device))

            orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]
            col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y
            col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x

            box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()

            num_masks = box.shape[0]
            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)
            sparse_embeddings, dense_embeddings = sam.prompt_encoder(
              points=None,
              boxes= box_torch,
              masks = None
            )

            low_res_masks, iou_predictions = sam.mask_decoder(
              image_embeddings=image_embedding,
              image_pe=sam.prompt_encoder.get_dense_pe(),
              sparse_prompt_embeddings=sparse_embeddings,
              dense_prompt_embeddings=dense_embeddings,
              multimask_output=False
            )

            upscaled_masks = sam.postprocess_masks(low_res_masks, input_size[i], original_image_size[i])

            binary_mask = torch.sigmoid(upscaled_masks.detach().cpu())
            binary_mask =  (binary_mask>0.5).float()

            gt_binary_mask = mask[i].detach().cpu()

            if binary_mask.size()[0] > 1:
                binary_mask = torch.unsqueeze(torch.sum(binary_mask, 0) / binary_mask.size()[0],0)

            dice.reset()
            gd.reset()
            iou.reset()

            dice(binary_mask[0,:], gt_binary_mask.unsqueeze(0))
            gd(binary_mask[0,:], gt_binary_mask.unsqueeze(0))
            iou(binary_mask[0,:], gt_binary_mask.unsqueeze(0))
            final_dice = dice.aggregate().numpy()[0]
            final_gd = gd.aggregate().numpy()[0]
            final_iou = iou.aggregate().numpy()[0]
            batch_dice.append(final_dice)
            batch_gd.append(final_gd)
            batch_iou.append(final_iou)


    print(f'Mean val dice: {sum(batch_dice) / len(batch_dice)}')
    print(f'Mean val gd: {sum(batch_gd) / len(batch_gd)}')
    print(f'Mean val iou: {sum(batch_iou) / len(batch_iou)}')

from monai.losses import GeneralizedDiceLoss, DiceLoss, GeneralizedDiceFocalLoss

# root_dir = 'D:\\Yuheng Li\\Segment Anything\\Model results\\SAM Finetune Dec'
root_dir = 'D:\\Yuheng Li\\Segment Anything\\Model results\\All datasets\\SAM Finetune Dec'

os.makedirs(root_dir, exist_ok = True)

# Set up the optimizer, hyperparameter tuning will improve performance here
lr = 4e-6
wd = 1e-4

num_epochs = 30

torch.backends.cudnn.benchmark = True
scaler = torch.cuda.amp.GradScaler()

optimizer = torch.optim.AdamW(sam.mask_decoder.parameters(), lr=lr, weight_decay=wd)

scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=30, max_epochs=num_epochs, warmup_start_lr=5e-7, eta_min = 1e-6)

# Freeze something

for param in sam.prompt_encoder.parameters():
    param.requires_grad = False

for param in sam.image_encoder.parameters():
    param.requires_grad = False

# loss_fn = GeneralizedDiceLoss(to_onehot_y=False)
loss_fn = DiceLoss(to_onehot_y=False)


from statistics import mean

from tqdm import tqdm
from torch.nn.functional import threshold, normalize

losses = []
dice_score = []
gd_score = []

best_dice = -1
best_score = -1
best_gd = -1

for epoch in range(num_epochs):

    epoch_losses = []

    for batch in train_dataloader:

        img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]

        batch_loss = 0

        for i in range(len(mask)):
            with torch.cuda.amp.autocast():

                with torch.no_grad():

                    image_embedding = sam.image_encoder(img[i].unsqueeze(0).to(device))

                    orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]
                    col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y
                    col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x
                    box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()

                    box_torch = torch.as_tensor(box, dtype=torch.float, device=device)

                    sparse_embeddings, dense_embeddings = sam.prompt_encoder(
                      points=None,
                      boxes=box_torch,
                      masks=None,
                    )

                low_res_masks, iou_predictions = sam.mask_decoder(
                  image_embeddings=image_embedding,
                  image_pe=sam.prompt_encoder.get_dense_pe(),
                  sparse_prompt_embeddings=sparse_embeddings,
                  dense_prompt_embeddings=dense_embeddings,
                  multimask_output=False,
                )

                upscaled_masks = sam.postprocess_masks(low_res_masks, input_size[i], original_image_size[i]).to(device)

                binary_mask = torch.sigmoid(upscaled_masks)

                gt_binary_mask = mask[i].to(device)

                if binary_mask.size()[0] > 1:
                    binary_mask = torch.unsqueeze(torch.sum(binary_mask, 0) / binary_mask.size()[0],0)

                loss = loss_fn(binary_mask[0], gt_binary_mask.unsqueeze(0)) / len(mask)
                scaler.scale(loss).backward()
                batch_loss += loss.item()

#         optimizer.zero_grad()
#         loss.backward()
#         optimizer.step()
        scaler.step(optimizer)
        scaler.update()
        epoch_losses.append(batch_loss / len(mask))
        optimizer.zero_grad()

    scheduler.step()
    losses.append(mean(epoch_losses))
    print(f'EPOCH: {epoch}')
    print(f'Mean loss: {mean(epoch_losses)}')


    with torch.no_grad():
        batch_dice = []
        batch_gd = []

        for batch in val_dataloader:

            img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]

            dice = DiceMetric()
            gd =  GeneralizedDiceScore()

            for i in range(len(mask)):
                image_embedding = sam.image_encoder(img[i].unsqueeze(0).to(device))
                orig_x, orig_y =  original_image_size[i][0], original_image_size[i][1]
                col_x1, col_x2 = bbox[i][:,1] * 1024/orig_y, bbox[i][:,3]* 1024/orig_y
                col_y1, col_y2 = bbox[i][:,0]* 1024/orig_x, bbox[i][:,2]* 1024/orig_x
                box = np.array([col_x1, col_y1, col_x2, col_y2]).transpose()

                num_masks = box.shape[0]
                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)
                sparse_embeddings, dense_embeddings = sam.prompt_encoder(
                  points=None,
                  boxes= box_torch,
                  masks = None
                )

                low_res_masks, iou_predictions = sam.mask_decoder(
                  image_embeddings=image_embedding,
                  image_pe=sam.prompt_encoder.get_dense_pe(),
                  sparse_prompt_embeddings=sparse_embeddings,
                  dense_prompt_embeddings=dense_embeddings,
                  multimask_output=False
                )

                upscaled_masks = sam.postprocess_masks(low_res_masks, input_size[i], original_image_size[i])

                binary_mask = torch.sigmoid(upscaled_masks.detach().cpu())
                binary_mask =  (binary_mask>0.5).float()

                gt_binary_mask = mask[i].detach().cpu()

                if binary_mask.size()[0] > 1:
                    binary_mask = torch.unsqueeze(torch.sum(binary_mask, 0) / binary_mask.size()[0],0)

                dice.reset()
                gd.reset()

                dice(binary_mask[0,:], gt_binary_mask.unsqueeze(0))
                gd(binary_mask[0,:], gt_binary_mask.unsqueeze(0))
                final_dice = dice.aggregate().numpy()[0]
                final_gd = gd.aggregate().numpy()[0]
                batch_dice.append(final_dice)
                batch_gd.append(final_gd)

        if (sum(batch_dice) / len(batch_dice)) > best_dice:
            best_dice = sum(batch_dice) / len(batch_dice)
            torch.save(
                sam.mask_decoder.state_dict(),
                os.path.join(root_dir, "best_dice_model_DL.pth"),
            )
            print("saved new best dice model")

        if (sum(batch_gd) / len(batch_gd)) > best_gd:
            best_gd = sum(batch_gd) / len(batch_gd)
            torch.save(
                sam.mask_decoder.state_dict(),
                os.path.join(root_dir, "best_GD_model_DL.pth"),
            )
            print("saved new best GD model")

        dice_score.append(sum(batch_dice) / len(batch_dice))
        gd_score.append(sum(batch_gd) / len(batch_gd))

    print(f'Mean val dice: {dice_score[-1]}')
    print(f'Mean val gd: {gd_score[-1]}')

import torch
from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score
import numpy as np
from segment_anything import SamPredictor  # Import SAM predictor

# Initialize SAM predictor
predictor = SamPredictor(sam)  # sam should be the fine-tuned SAM model

# Function to calculate all metrics
def calculate_metrics(y_true, y_pred):
    y_true = y_true.flatten()
    y_pred = y_pred.flatten()

    precision = precision_score(y_true, y_pred, average='binary')
    recall = recall_score(y_true, y_pred, average='binary')
    f1 = f1_score(y_true, y_pred, average='binary')
    iou = jaccard_score(y_true, y_pred, average='binary')  # IoU

    return {
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1,
        'IoU': iou
    }

sam.eval()  # Set the model to evaluation mode

metrics = {
    'Precision': [],
    'Recall': [],
    'F1 Score': [],
    'IoU': []
}

# Iterate through the validation/test set in smaller batches
batch_size = 2  # Adjust as needed based on available GPU memory
for batch in val_dataloader:
    img, bbox, mask, gt_resized, original_image_size, input_size = batch[0], batch[1], batch[2], batch[3], batch[4], batch[5]

    # Split the batch into smaller chunks
    for i in range(0, len(img), batch_size):
        images = img[i:i+batch_size].to('cuda')  # Move images to GPU
        masks = [m.to('cuda') for m in mask[i:i+batch_size]]  # Move masks to GPU

        with torch.no_grad():
            # Get image embedding from SAM's image encoder
            image_embedding = sam.image_encoder(images)

            # Generate masks using SAM predictor
            predictor.set_image(images, image_embedding)

            # SAM prediction for each image in the chunk
            preds = []
            for j in range(len(images)):
                # Generate the mask based on the bounding box (bbox) and original size
                predicted_masks, _, _ = predictor.predict_torch(
                    point_coords=None,  # No point inputs used here
                    point_labels=None,  # No labels used here
                    boxes=bbox[i+j],  # Bounding box for the current image
                    multimask_output=False  # Set to False for a single mask
                )
                preds.append(predicted_masks[0])  # Append the predicted mask for this image

            # Calculate metrics
            for j in range(len(images)):
                result = calculate_metrics(masks[j].cpu().numpy(), preds[j].cpu().numpy())
                for key in metrics:
                    metrics[key].append(result[key])

# Aggregate results
for key in metrics:
    metrics[key] = np.mean(metrics[key])

print(metrics)

plt.figure(figsize=(5,5))
plt.imshow(binary_mask.detach().cpu().numpy()[0,0])
plt.axis('off')
plt.show()

plt.figure(figsize=(5,5))
plt.imshow(gt_binary_mask.detach().cpu().numpy())
plt.axis('off')
plt.show()

plt.figure("train", (18, 6))
plt.subplot(1, 3, 1)
plt.title("Epoch Average Loss")
x = [i + 1 for i in range(len(losses))]
y = losses
plt.xlabel("epoch")
plt.plot(x, y, color="red")
plt.subplot(1, 3, 2)
plt.title("Val Mean Dice")
x = [(i + 1) for i in range(len(dice_score))]
y = dice_score
plt.xlabel("epoch")
plt.plot(x, y, color="green")
plt.subplot(1, 3, 3)
plt.title("Val Generalized Dice")
x = [(i + 1) for i in range(len(gd_score))]
y = gd_score
plt.xlabel("epoch")
plt.plot(x, y, color="green")
plt.show()

lr = lr[-1]
max_epochs = 200
optimizer = torch.optim.AdamW(sam.mask_decoder.parameters(), lr=lr, weight_decay=wd)
scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=30, max_epochs=num_epochs, warmup_start_lr=5e-7, eta_min = 1e-6)

lr = []

for i in range(max_epochs):
    scheduler.step()
    lr.append(optimizer.param_groups[0]['lr'])

plt.figure("train", (6, 6))
plt.title("Epoch LR")
x = [i + 1 for i in range(max_epochs)]
y = lr
plt.xlabel("epoch")
plt.plot(x, y, color="red")

plt.figure(figsize=(5,5))
plt.imshow(binary_mask.detach().cpu().numpy()[0,0])
plt.axis('off')
plt.show()

plt.figure(figsize=(5,5))
plt.imshow(gt_binary_mask.detach().cpu().numpy())
plt.axis('off')

plt.show()
print(binary_mask.detach().cpu().numpy()[0,0])

print(np.max(binary_mask.detach().cpu().numpy()[0,0]))

transformed_data = defaultdict(dict)

for i in range(0, 3):

    idx = image_path[i].split('images\\')[1].split('.jpg')[0]
    bbox = labels[idx]['bbox']

    bbox_arr = []
    x = [i['xmin'] for i in labels[idx]['bbox']]
    y = [i['ymin']for i in labels[idx]['bbox']]
    xx = [i['xmax'] for i in labels[idx]['bbox']]
    yy = [i['ymax'] for i in labels[idx]['bbox']]

    for k in range(len(bbox)):

        bbox_arr.append(np.array([x[k], y[k], xx[k], yy[k]]))

    bbox_arr = np.array(bbox_arr)

    image = cv2.imread(image_path[i])
    gt = cv2.imread(mask_path[i])
    gt = cv2.cvtColor(gt, cv2.COLOR_BGR2GRAY) / 255
    gt = gt.astype('float32')

    transform = ResizeLongestSide(sam.image_encoder.img_size)
    input_image = transform.apply_image(image)
    print(input_image.shape)

    input_image_torch = torch.as_tensor(input_image, device=device)
    transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]
    print(transformed_image.shape)

    input_image = sam.preprocess(transformed_image)
    print(input_image.shape)

    original_image_size = image.shape[:2]
    input_size = tuple(transformed_image.shape[-2:])

    transformed_data[i]['image'] = input_image
    transformed_data[i]['input_size'] = input_size
    transformed_data[i]['original_image_size'] = original_image_size
    transformed_data[i]['bbox_arr'] = bbox_arr